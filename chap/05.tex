\chapter{优化程序性能}

编写高效程序需要做到以下几点：第一，我们必须选择一组适当的算法和数据结构。第二，我们必须编写出编译器能够有效优化以转换成高效可执行代码的源代码。第三，针对处理运算量特别大的计算，将一个任务分成多个部分，这些部分可以在多核和多处理器的某种组合上并行地计算。对于第二点，理解优化编译器的能力和局限性是很重要的。

理想的情况是，编译器能够接受我们编写的任何代码，并产生尽可能高效的、具有指定行为的机器级程序。现代编译器采用了复杂的分析和优化形式，然而，即使是最好的编译器也受到 optimization blocker 的阻碍。程序员必须编写容易优化的代码，以帮助编译器。

程序优化的第一步就是消除不必要的工作，让代码尽可能有效地执行所期望的任务。这包括消除不必要的函数调用、内存条件测试和内存引用。这些优化不依赖于目标机器的任何具体属性。为了使程序性能最大化，程序员和编译器都需要一个目标机器的模型，指明如何处理指令，以及各个操作的时序特性。了解了处理器的运作，我们就可以进行程序优化的第二部，利用处理器提供的 instruction-level parallelism 能力，同时执行多条指令。

研究程序的汇编代码表示是理解编译器以及产生的代码会如何运行的最有效手段之一。仔细研究内循环的代码是一个很好的开端，识别出降低性能的属性，例如过多的内存引用和对寄存器使用不当。从汇编代码开始，我们还可以预测什么操作会并行执行，以及它们会如何使用处理器资源。常常会通过确认 critical path 来决定执行一个循环所需要的时间。

\section{优化编译器的能力和局限性}

现代编译器运用复杂精细的算法来确定一个程序中计算的是什么值，以及它们是被如何使用的，然后利用一些机会来简化表达式。

编译器必须很小心地对程序只使用安全的优化，也就是说对于程序可能遇到的所有可能的情况，优化后得到的程序和未优化的版本有一样的行为。例如：

\begin{cppcode}
void twiddle1(long *xp, long *yp) {
  *xp += *yp;
  *xp += *yp;
}

void twiddle2(long *xp, long *yp) {
  *xp += 2 * *yp;
}
\end{cppcode}

看起来，这两个过程似乎有相同的行为。不过考虑 xp 等于 yp 的情况。此时，函数 twiddle1 会执行下面的计算：

\begin{cppcode*}{autogobble=false, firstnumber=2}
  *xp += *xp;  // Double value at xp
  *xp += *xp;  // Double value at xp
\end{cppcode*}
结果是 xp 的值增加 4 倍。另一方面，函数 twiddle2 会执行下面的计算：

\begin{cppcode*}{autogobble=false, firstnumber=7}
  *xp += 2 * *xp;  // Triple value at xp
\end{cppcode*}
结果是 xp 的值增加 3 倍。编译器不知道 twiddle 会如何被调用，因此它必须假设参数 xp 和 yp 可能会相等。因此，它不能产生 twiddle2 风格的代码作为 twiddle1 的优化版本。

这种两个指针可能指向同一个内存位置的情况称为 memory aliasing。在只执行安全的优化中，编译器必须假设不同的指针可能会指向内存中同一个位置。

这造成了一个主要的 optimization blocker，这也是可能严重限制编译器产生优化代码机会的程序的一个方面。如果编译器不能确定两个指针是否指向同一个位置，就必须假设什么情况都有可能，这就限制了可能的优化策略。

\section{表示程序性能}

我们引入度量标准 Cycles Per Element，CPE，作为一种表示程序性能并指导我们改进代码的方法。CPE 这种度量标准帮助我们在更细节的级别上理解迭代程序的循环性能。

\section{程序示例}

为了说明一个抽象的程序是如何被系统地转换成更有效的代码的，我们将使用一个基于下图所示向量数据结构的运行示例。

\begin{tikzfig}
    \coordinate (A) at (0, 0);
    \coordinate (B) at (2, 0);
    \coordinate (C) at (0, -1);
    \coordinate (D) at (B |- C);
    \coordinate (ac1) at ($(A)!1/2!(C)$);
    \coordinate (bd1) at ($(B)!1/2!(D)$);
    \fill[fill=White!80!ProcessBlue] (A) rectangle (D);
    \path (A) rectangle node{len} (bd1);
    \draw (ac1) -- (bd1);
    \draw (A) -- node[left]{len} (ac1) -- node[left]{data} (C) -- (D) -- (B) -- (A);
    \coordinate (E) at ($(bd1) + (2, 0)$);
    \coordinate (F) at ($(E) + (7, 0)$);
    \coordinate (G) at ($(D) + (2, 0)$);
    \coordinate (H) at ($(G) + (7, 0)$);
    \foreach \i in {0, 1, 2, ..., 7} {
        \coordinate (ef\i) at ($(E)!\i/7!(F)$);
        \coordinate (gh\i) at ($(G)!\i/7!(H)$);
    }
    \fill[fill=White!60!gray] (E) rectangle (gh3);
    \fill[fill=White!60!gray] (ef6) rectangle (H);
    \draw (E) -- node[above]{0} (ef1)
              -- node[above]{1} (ef2)
              -- node[above]{2} (ef3)
              -- (ef6)
              -- node[above]{len-1} (F)
              -- (H) -- (G) -- (E);
    \draw (ef1) -- (gh1);
    \draw (ef2) -- (gh2);
    \draw (ef3) -- (gh3);
    \draw (ef6) -- (gh6);
    \path (ef3) rectangle node{. . .} (gh6);
    \coordinate (K) at ($(bd1)!1/2!(D) - (0.2, 0)$);
    \fill (K) circle[radius=1pt];
    \draw[-latex] (K) -- ($(E)!1/2!(G)$);
\end{tikzfig}

向量由两个内存块表示：头部和数据数组。头部是一个声明如下的结构：

\begin{cppcode}
// vec.h
// Create abstract data type for vector
typedef struct {
  long len;
  data_t *data;
} vec_rec, *vec_ptr;
\end{cppcode}

这个声明用 \verb|data_t| 来表示基本元素的数据类型。在测试中，我们度量代码对于整数和浮点数数据的性能。

下面给出了一些生成向量、访问向量元素以及确定向量长度的基本过程：

\begin{cppcode}
// Create vector of specified length
vec_ptr new_vec(long len) {
  // Allocate header structure
  vec_ptr result = (vec_ptr)malloc(sizeof(vec_rec));
  data_t *data = NULL;
  if (!result) return NULL; // Couldn't allocate storage
  result->len = len;
  // Allocate array
  if (len > 0) {
    data = (data_t*)calloc(len, sizeof(data_t));
    if (!data) {
      free((void*)result);
      return NULL; // Couldn't allocate storage
    }
  }
  // Data will either be NULL or allocated storage
  result->data = data;
  return result;
}

// Retrieve vector element and store at dest
// Return 0 (out of bounds) or 1 (successful)
int get_vec_element(vec_ptr v, long index, data_t *dest) {
  if (index < 0 || index >= v->len) return 0;
  *dest = v->data[index];
  return 1;
}

// Return length of vector
long vec_length(vec_ptr v) {
  return v->len;
}
\end{cppcode}

作为一个优化示例，考虑如下代码：

\begin{cppcode}
// Implementation with maximum use of data abstraction
void combine1(vec_ptr v, data_t *dest) {
  long i;
  *dest = IDENT;
  for (i = 0; i < vec_length(v); i++) {
    data_t val;
    get_vec_element(v, i, &val);
    *dest = *dest OP val;
  }
}
\end{cppcode}

它使用某种运算，将一个数组中所有的元素合并成一个值。通过使用编译时常数 IDENT 和 OP 的不同定义，这段代码可以重编译成对数据执行不同的运算。特别的，使用声明：
\begin{cppcode}
#define IDENT 0
#define OP +
\end{cppcode}
它对数组的元素求和。使用声明：
\begin{cppcode}
#define IDENT 1
#define OP *
\end{cppcode}
它计算的是数组元素的乘积。

作为一个起点，下表给出的是 combine1 的 CPE 度量值，它运行在参考机上，尝试了操作（加法或乘法）和数据类型（整数或浮点数）的不同组合。

\begin{table}[!ht]
    \centering
    \begin{tabular}{llccccc}
        \toprule
         \multirow{2}{*}{Function} & \multirow{2}{*}{Method} & \multicolumn{2}{c}{Integer} & & \multicolumn{2}{c}{Floating point} \\
         \cmidrule{3-4}
         \cmidrule{6-7}
          & & + & * & & + & * \\
         \midrule
         \texttt{combine1} & Abstract unoptimized & 22.68 & 20.02 & & 19.98 & 20.18 \\
         \texttt{combine1} & Abstract -O1 & 10.12 & 10.12 & & 10.17 & 11.14 \\
         \bottomrule
    \end{tabular}
\end{table}

\section{消除循环的低效率}

可以观察到，过程 combine1 调用函数 \verb|vec_length| 作为 for 循环的测试条件，每次循环迭代时都必须对测试条件求值。另一方面，向量的长度并不会随着循环的进行而改变。因此，只需计算一次向量的长度。

\begin{cppcode}
// Move call to vec_length out of loop
void combine2(vec_ptr v, data_t *dest) {
  long i;
  long length = vec_length(v);
  *dest = IDENT;
  for (i = 0; i < length; i++) {
    data_t val;
    get_vec_element(v, i, &val);
    *dest = *dest OP val;
  }
}
\end{cppcode}

\begin{table}[!ht]
    \centering
    \begin{tabular}{llccccc}
        \toprule
         \multirow{2}{*}{Function} & \multirow{2}{*}{Method} & \multicolumn{2}{c}{Integer} & & \multicolumn{2}{c}{Floating point} \\
         \cmidrule{3-4}
         \cmidrule{6-7}
          & & + & * & & + & * \\
         \midrule
         \texttt{combine1} & Abstract -O1 & 10.12 & 10.12 & & 10.17 & 11.14 \\
         \texttt{combine2} & Move \texttt{vec\_length} & 7.02 & 9.03 & & 9.02 & 11.03 \\
         \bottomrule
    \end{tabular}
\end{table}

这个优化是一类常见的优化，称为 code motion（代码移动）。这类优化包括识别要执行多次但是计算结果不会改变的计算。因而可以将计算移动到代码前面不会被多次求值的部分。

\section{减少过程调用}

过程调用会带来开销，而且妨碍大多数形式的程序优化。从 combine2 的代码中我们可以看出，每次循环迭代都会调用 \verb|get_vec_element| 来获取下一个向量元素。对每个向量引用，这个函数要把向量索引 i 与循环边界做比较，很明显会降低效率。在处理任意的数组访问时，边界检查很有用，但是对 combine2 代码的简单分析表明，所有的引用都是合法的。

作为替代，假设为我们的抽象数据类型增加一个函数 \verb|get_vec_start|。这个函数返回数组的起始地址，然后就能写出下面的 combine3 所示的过程，其内循环里没有函数调用。它没有用函数调用来获取每个向量元素，而是直接访问数组。

\begin{cppcode}
data_t *get_vec_start(vec_ptr v) {
  return v->data;
}
\end{cppcode}

\begin{cppcode}
// Direct access to vector data
void combine3(vec_ptr v, data_t *dest) {
  long i;
  long length = vec_length(v);
  data_t *data = get_vec_start(v);
  *dest = IDENT;
  for (i = 0; i < length; i++) {
    *dest = *dest OP data[i];
  }
}
\end{cppcode}

\begin{table}[!ht]
    \centering
    \begin{tabular}{llccccc}
        \toprule
         \multirow{2}{*}{Function} & \multirow{2}{*}{Method} & \multicolumn{2}{c}{Integer} & & \multicolumn{2}{c}{Floating point} \\
         \cmidrule{3-4}
         \cmidrule{6-7}
          & & + & * & & + & * \\
         \midrule
         \texttt{combine2} & Move \texttt{vec\_length} & 7.02 & 9.03 & & 9.02 & 11.03 \\
         \texttt{combine3} & Direct data access & 7.17 & 9.02 & & 9.02 & 11.03 \\
         \bottomrule
    \end{tabular}
\end{table}

令人吃惊的是，性能没有明显的提升。事实上，整数求和的性能还略有下降。显然，内循环中的其他操作形成了瓶颈，限制性能超过调用 \verb|get_vec_element|。我们还会再回到这个函数（5.11.2 节），看看为什么 combine2 中反复的边界检查不会让性能更差。

\section{消除不必要的内存引用}

combine3 的代码将合并运算计算的值累积在指针 \verb|dest| 指定的位置。通过检查编译出来的为内循环产生的汇编代码，可以看出这一行为。在此我们给出数据类型为 double，合并运算为乘法的 x86-64 代码：

\begin{gascode}
# Inner loop of combine3. data_t = double, OP = *
# dest in %rbx, data in %rax, i in %rcx, length in %r14
.LBB6_2:                            # loop:
    movsd   (%rbx), %xmm0           #   Read product from dest
    mulsd   (%rax,%rcx,8), %xmm0    #   Multiply product by data[i]
    movsd   %xmm0, (%rbx)           #   Store product at dest
    addq    $1, %rcx                #   Increment i
    cmpq    %rcx, %r14              #   Comapre to length
    jne .LBB6_2                     #   if !=, goto loop
\end{gascode}

可以看到每次迭代时，累积变量的数值都要从内存读出再写入到内存。这样的读写很浪费，因为每次迭代开始时从 \verb|dest| 读出的值就是上次迭代最后写入的值。我们能够消除这种不必要的内存读写，按照如下 combine4 所示的方式重写代码。引入一个临时变量 \verb|acc|，它在循环中用来累积计算出来的值。只有在循环完成之后才将结果存放到 \verb|dest| 中。

\begin{cppcode}
// Accumulate result in local variable
void combine4(vec_ptr v, data_t *dest) {
  long i;
  long length = vec_length(v);
  data_t *data = get_vec_start(v);
  data_t acc = IDENT;
  for (i = 0; i < length; i++) {
    acc = acc OP data[i];
  }
  *dest = acc;
}
\end{cppcode}

\begin{gascode}
# Inner loop of combine4. data_t = double, OP = *
# acc in %xmm0, data in %rax, i in %rcx, length in %rbx
.LBB7_2:                            # loop:
    mulsd   (%rax,%rcx,8), %xmm0    #   Multiply acc by data[i]
    addq    $1, %rcx                #   Increment i
    cmpq    %rcx, %rbx              #   Compare to length
    jne .LBB7_2                     #   if !=, goto loop
\end{gascode}

与 combine3 中的循环相比，我们将每次迭代的内存操作从两次读和一次写减少到只需要一次读。

\begin{table}[!ht]
    \centering
    \begin{tabular}{llccccc}
        \toprule
         \multirow{2}{*}{Function} & \multirow{2}{*}{Method} & \multicolumn{2}{c}{Integer} & & \multicolumn{2}{c}{Floating point} \\
         \cmidrule{3-4}
         \cmidrule{6-7}
          & & + & * & & + & * \\
         \midrule
         \texttt{combine3} & Direct data access & 7.17 & 9.02 & & 9.02 & 11.03 \\
         \texttt{combine4} & Accumulate in temporary & 1.27 & 3.01 & & 3.01 & 5.01 \\
         \bottomrule
    \end{tabular}
\end{table}

需要注意的是，由于内存别名问题，combine3 和 combine4 在某些情况可能会有不同的行为，所以编译器会采用保守的策略，不会自动将 combine3 优化为 combine4，，尽管 combine4 的行为更加符合函数描述的意图。

当用带命令行选项 -O2 编译 combine3 时，得到的代码 CPE 性能远好于使用 -O1 时：

\begin{table}[!ht]
    \centering
    \begin{tabular}{llccccc}
        \toprule
         \multirow{2}{*}{Function} & \multirow{2}{*}{Method} & \multicolumn{2}{c}{Integer} & & \multicolumn{2}{c}{Floating point} \\
         \cmidrule{3-4}
         \cmidrule{6-7}
          & & + & * & & + & * \\
         \midrule
         \texttt{combine3} & Compiled -O1 & 7.17 & 9.02 & & 9.02 & 11.03 \\
         \texttt{combine4} & Compiled -O2 & 1.60 & 3.01 & & 3.01 & 5.01 \\
         \texttt{combine4} & Accumulate in temporary & 1.27 & 3.01 & & 3.01 & 5.01 \\
         \bottomrule
    \end{tabular}
\end{table}

在检查编译器产生的汇编代码时，我们发现内循环的一个有趣的变化：

\begin{gascode}
# Inner loop of combine3. data_t = double, OP = *. Compiled -O2
# dest in %rsi, data in %rcx, i in %rdx, length in %rax
.LBB6_7:                            # loop:
    mulsd   (%rcx,%rdx,8), %xmm0    #   Multiply product by data[i]
    movsd   %xmm0, (%rsi)           #   Store product at dest
    addq    $1, %rdx                #   Increment i
    cmpq    %rdx, %rax              #   Compare to length
    jne .LBB6_7                     #   if !=, goto loop
\end{gascode}

把上面的代码与用 -O1 产生的代码进行比较：

\begin{gascode}
# Inner loop of combine3. data_t = double, OP = *. Compiled -O1
# dest in %rbx, data in %rax, i in %rcx, length in %r14
.LBB6_2:                            # loop:
    movsd   (%rbx), %xmm0           #   Read product from dest
    mulsd   (%rax,%rcx,8), %xmm0    #   Multiply product by data[i]
    movsd   %xmm0, (%rbx)           #   Store product at dest
    addq    $1, %rcx                #   Increment i
    cmpq    %rcx, %r14              #   Comapre to length
    jne .LBB6_2                     #   if !=, goto loop
\end{gascode}

我们看到，-O2 中减少了一条 \verb|movsd| 指令，直接从 \verb|dest| 指定的位置读数据。

\section{理解现代处理器}

到目前为止，我们运用的优化都不依赖于目标机器的任何特性。这些优化只是简单地降低了过程调用的开销，以及消除了一些重大的 optimization blocker，这些因素会给优化编译器造成困难。随着试图进一步提高性能，必须考虑利用处理器微体系结构的优化，也就是处理器用来执行指令的底层系统设计。要想充分提高性能，需要仔细分析程序，同时代码的生成也要针对目标处理器进行调整。

为了理解改进性能的方法，我们需要理解现代处理器的微体系结构。在代码级上，看上去似乎是一次执行一条指令，每条指令都包括从寄存器或内存取值，执行一个操作，并把结果存回到一个寄存器或内存位置。在实际的处理器中，是同时对多条指令求值的，这个现象称为指令级并行。

我们会发现两种下界描述了程序的最大性能。当一系列操作必须按照严格顺序执行时，就会遇到 latency bound（延迟界限），因为在下一条指令开始之前，这条指令必须结束。当代码中的数据相关限制了处理器利用指令级并行的能力时，延迟界限能够限制程序性能。throughput bound（吞吐量界限）刻画了处理器功能单元的原始计算能力，这个界限是程序性能的终极限制。
